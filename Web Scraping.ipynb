{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import os\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from googletrans import Translator"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_up_driver(src: str) -> webdriver.Chrome:\n",
    "    # Create ChromeOptions object and add headless mode argument\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--headless\")\n",
    "\n",
    "    # Create Chrome WebDriver instance with specified options and load the URL\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    driver.get(src)\n",
    "\n",
    "    return driver\n",
    "\n",
    "\n",
    "def get_soup(driver: webdriver.Chrome) -> BeautifulSoup:\n",
    "    # Create BeautifulSoup object from the page source of the WebDriver instance\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "    return soup\n",
    "\n",
    "\n",
    "def extract_js_files(soup: BeautifulSoup) -> list:\n",
    "    # Extract the 'src' attribute from all 'script' tags in the HTML and return as a list\n",
    "    js_files = [js.get(\"src\") for js in soup.find_all(\"script\") if js.get(\"src\")]\n",
    "\n",
    "    return js_files\n",
    "\n",
    "\n",
    "def include_js_files(soup: BeautifulSoup, js_files: list) -> None:\n",
    "    # Create a new 'script' tag with the 'src' attribute for each JS file in the list\n",
    "    # and append it to the 'head' tag of the HTML\n",
    "    for js in js_files:\n",
    "        script_tag = soup.new_tag(\"script\", src=js)\n",
    "        soup.head.append(script_tag)\n",
    "\n",
    "\n",
    "def translate_html_file(soup, translated_lang):\n",
    "    # Create Translator object from the 'googletrans' library\n",
    "    translator = Translator()\n",
    "\n",
    "    # Loop through all tags in the HTML and translate any text content in the specified language\n",
    "    for tag in soup.recursiveChildGenerator():\n",
    "        if tag.name not in ['style', 'script'] and hasattr(tag, 'text') and tag.string:\n",
    "            translated = translator.translate(text=tag.text, dest=translated_lang).text\n",
    "            tag.string.replace_with(translated)\n",
    "\n",
    "    return soup\n",
    "\n",
    "\n",
    "def save_html_content(soup: BeautifulSoup, dst: str) -> None:\n",
    "    # Replace any '/' characters in the destination filename with underscores\n",
    "    dst = dst.replace('/', '_')\n",
    "\n",
    "    # Convert the BeautifulSoup object to an HTML string and write it to a file\n",
    "    html_string = str(soup)\n",
    "    with open(f\"{dst}.html\", \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(html_string)\n",
    "\n",
    "\n",
    "def close_driver(driver: webdriver.Chrome) -> None:\n",
    "    # Quit the WebDriver instance to close the Chrome window and free up resources\n",
    "    driver.quit()\n",
    "\n",
    "\n",
    "def scrap_translate_webpage(src:str, dst:str, translated_lang:str) -> None:\n",
    "    # Set up Chrome WebDriver instance and load the source URL\n",
    "    driver = set_up_driver(src)\n",
    "\n",
    "    # Create BeautifulSoup object from the WebDriver's page source\n",
    "    soup = get_soup(driver)\n",
    "\n",
    "    # Extract all JavaScript files in the page and add them to the soup\n",
    "    js_files = extract_js_files(soup)\n",
    "    include_js_files(soup, js_files)\n",
    "\n",
    "    # Translate the HTML content using the specified language and update the soup\n",
    "    soup_tarns = translate_html_file(soup, translated_lang)\n",
    "\n",
    "    # Save the translated HTML content to a file with the specified destination\n",
    "    save_html_content(soup_tarns, dst)\n",
    "\n",
    "    # Close the Chrome WebDriver instance\n",
    "    close_driver(driver)\n",
    "\n",
    "\n",
    "def extract_href_links(soup: BeautifulSoup, website_url) -> list:\n",
    "    # Extract all href links from the page, remove any links containing \"https\", \"http\", \"mailto\", or \"/\"\n",
    "    href_links = [a.get(\"href\") for a in soup.find_all(\"a\") if a.get(\"href\")]\n",
    "    href_links = [link.replace(website_url, \"\") for link in href_links]\n",
    "    href_links = [elem for elem in href_links if elem != '/']\n",
    "    href_links = [item for item in href_links if not any(substring in item for substring in ['https', 'http', 'mailto'])]\n",
    "\n",
    "    return href_links\n",
    "\n",
    "\n",
    "def preprocess_anchor_src(src: str, dst: str, website_url:str) -> None:\n",
    "    # Open the source HTML file and create a BeautifulSoup object\n",
    "    with open(f'{src}.html', 'r', encoding='utf-8') as file:\n",
    "        html_content = file.read()\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Update all anchor links in the soup to point to a file with a new name, replacing \"/\" with \"_\"\n",
    "    for anchor in soup.find_all('a'):\n",
    "        href = anchor.get('href')\n",
    "        href = href.replace(website_url, \"\")\n",
    "        if href and not any(substring in href for substring in ['https', 'http', 'mailto']) and href != '/':\n",
    "            href = href.replace('/', '_')\n",
    "            href += '.html'\n",
    "            anchor['href'] = href\n",
    "\n",
    "    # Save the updated HTML content to a file with the specified destination and delete the source file\n",
    "    updated_html_content = str(soup)\n",
    "    with open(f'{dst}.html', 'w', encoding='utf-8') as file:\n",
    "        file.write(updated_html_content)\n",
    "    os.remove(f'{src}.html')\n",
    "\n",
    "\n",
    "def perform_one_layer_depth_and_get_links(website_url, final_page, translated_lang):\n",
    "    PROCESSED_PAGE = \"test\" # The processed page name\n",
    "\n",
    "    # Extract the main webpage and translate it\n",
    "    scrap_translate_webpage(website_url, PROCESSED_PAGE, TRANSLATED_LANG)\n",
    "\n",
    "    # Extract all the href links for the main webpage\n",
    "    with open(f'{PROCESSED_PAGE}.html', \"r\", encoding=\"utf-8\") as file:\n",
    "        soup = BeautifulSoup(file.read(), \"html.parser\")\n",
    "        href_links = extract_href_links(soup, website_url)\n",
    "\n",
    "    # Scrape all external nodes\n",
    "    for page in tqdm(href_links):\n",
    "        src = website_url+page\n",
    "        scrap_translate_webpage(src, page, translated_lang)\n",
    "\n",
    "    # Adjust external nodes' href links relative to the main node\n",
    "    preprocess_anchor_src(PROCESSED_PAGE, final_page, website_url)\n",
    "\n",
    "    return 1, href_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [05:51<00:00, 175.99s/it]\n"
     ]
    }
   ],
   "source": [
    "home_page_url = \"https://www.classcentral.com\"\n",
    "adjusted_page = \"index\" # The adjusted page name\n",
    "TRANSLATED_LANG = 'hi' # The language to which the webpage will be translated\n",
    "\n",
    "_, one_layer_depth_external_links = perform_one_layer_depth_and_get_links(home_page_url, adjusted_page, TRANSLATED_LANG)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # to perform two layer depth\n",
    "# two_layers_depth_external_links = [[]]  # resulted will be list of lists (2d) - for more general solution we will needed to have (n-1) loops for (n) layers\n",
    "# for link in one_layer_depth_external_links:\n",
    "#     _, two_layers_depth_external_links.append(perform_one_layer_depth_and_get_links(home_page_url+'/'+link, link.replace(\"/\",\"_\"), 'hi'))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
